class: middle, center, title-slide
# –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —ñ –ø—Ä–æ–µ–∫—Ç—É–≤–∞–Ω–Ω—è —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º

–õ–µ–∫—Ü—ñ—è 2: –£–≤–∞–≥–∞ —Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏

<br><br>
–ö–æ—á—É—Ä–∞ –Æ—Ä—ñ–π –ü–µ—Ç—Ä–æ–≤–∏—á<br>
[iuriy.kochura@gmail.com](mailto:iuriy.kochura@gmail.com) <br>
<a href="https://t.me/y_kochura">@y_kochura</a> <br>


---

class:  black-slide,
background-image: url(./figures/lec1/robot.png)

<br>

# –°—å–æ–≥–æ–¥–Ω—ñ

.larger-x[<p class="shadow">–£–≤–∞–≥–∞ &mdash; —Ü–µ –≤—Å–µ, —â–æ –í–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ!  <br><br>

 üéôÔ∏è –ö–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä <br>
 üéôÔ∏è –ú–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏ –ë–∞—Ö–¥–∞–Ω–∞—É (–∞–¥–∏—Ç–∏–≤–Ω–∞ —É–≤–∞–≥–∞) <br>
 üéôÔ∏è –®–∞—Ä —É–≤–∞–≥–∏ <br>
 üéôÔ∏è –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ <br>
  </p>]

???

–ó–∞–≤–¥–∞–Ω–Ω—è: –¥—ñ–∑–Ω–∞—Ç–∏—Å—è –ø—Ä–æ –Ω–æ–≤–∏–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫ —Å—É—á–∞—Å–Ω–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂. –¶–µ–π –±–ª–æ–∫ –º–æ–∂–µ –∑–∞–º—ñ–Ω–∏—Ç–∏ —è–∫ FC, —Ç–∞–∫ —ñ –∑–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏.

---

class: blue-slide, middle, center
count: false

.larger-xx[–ö–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä]

---


class: middle

–£ –±–∞–≥–∞—Ç—å–æ—Ö –ø—Ä–∏–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–æ–±–∫–∞ —Å–∏–≥–Ω–∞–ª—ñ–≤ –∑ .bold[–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é] —î –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—é.


- .bold[–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π]:
    - –∞–Ω–∞–ª—ñ–∑ –µ–º–æ—Ü—ñ–π —É —Ç–µ–∫—Å—Ç—ñ
    - —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ/–¥—ñ–π —É –≤—ñ–¥–µ–æ
    - –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –î–ù–ö
- .bold[–°–∏–Ω—Ç–µ–∑ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π]:
    - —Å–∏–Ω—Ç–µ–∑ —Ç–µ–∫—Å—Ç—É
    - —Å–∏–Ω—Ç–µ–∑ –º—É–∑–∏–∫–∏
    - —Å–∏–Ω—Ç–µ–∑ —Ä—É—Ö—É
- .bold[–ü–µ—Ä–µ–∫–ª–∞–¥ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π]:
    - —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏
    - –ø–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É
    - –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [14x050/EE559 Deep Learning](https://fleuret.org/dlc/), EPFL.]

---

class: middle

–ù–µ—Ö–∞–π $\mathcal{X}$ &mdash; —Ü–µ –º–Ω–æ–∂–∏–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–≤. –¢–æ–¥—ñ $S(\mathcal{X})$ &mdash; —Ü–µ –º–Ω–æ–∂–∏–Ω–∞ —É—Å—ñ—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –¥–æ–≤—ñ–ª—å–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏,
$$S(\mathcal{X}) = \\{(x\_1, x\_2, ..., x\_n) \mid n \ge 1, x\_i \in \mathcal{X}\\} =  \bigcup\_{n=1}^\infty \mathcal{X}^n,$$ —Ç–æ–¥—ñ –º–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ:


.grid.center[
.kol-1-2.bold[–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π]
.kol-1-2[$f: S(\mathcal{X}) \to \bigtriangleup^C$]
]
.grid.center[
.kol-1-2.bold[–°–∏–Ω—Ç–µ–∑ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π]
.kol-1-2[$f: \mathbb{R}^d \to S(\mathcal{X})$]
]
.grid.center[
.kol-1-2.bold[–ü–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å-–ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å]
.kol-1-2[$f: S(\mathcal{X}) \to S(\mathcal{Y})$]
]

.alert[–¢–æ–∫–µ–Ω &mdash; —Ü–µ –µ–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, —è–∫–∏–π –º–æ–¥–µ–ª—å –æ–±—Ä–æ–±–ª—è—î —è–∫ –æ–∫—Ä–µ–º—É –æ–¥–∏–Ω–∏—Ü—é.]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [EE559 Deep Learning](https://fleuret.org/ee559/), EPFL.]

---

class: middle

–ö–æ–ª–∏ –≤—Ö—ñ–¥–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏ —î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å $\mathbf{x} \in S(\mathbb{R}^p)$ –∑–º—ñ–Ω–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏, –∑–∞–∑–≤–∏—á–∞–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É .bold[¬´–∫–æ–¥–µ—Ä‚Äì–¥–µ–∫–æ–¥–µ—Ä¬ª], —è–∫–∞ —Å–ø–æ—á–∞—Ç–∫—É —Å—Ç–∏—Å–∫–∞—î –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –≤ —î–¥–∏–Ω–∏–π –≤–µ–∫—Ç–æ—Ä $v$, –∞ –ø–æ—Ç—ñ–º –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –π–æ–≥–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤–∏—Ö—ñ–¥–Ω–æ—ó –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ.

<br>

.center.width-85[![](figures/lec2/encoder-decoder.svg)]

*–ü—Ä–∏–∫–ª–∞–¥*
- .bold[–í—Ö—ñ–¥:] I am learning AI.
- .bold[–í–∏—Ö—ñ–¥:] –Ø –≤–∏–≤—á–∞—é –®–Ü.

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

???
–£ –∑–∞–¥–∞—á–∞—Ö —Ç–∏–ø—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å ‚Üí –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —É –º–∞—à–∏–Ω–Ω–æ–º—É –ø–µ—Ä–µ–∫–ª–∞–¥—ñ, –≤—Ö—ñ–¥–Ω—ñ —Ç–∞ –≤–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –º–æ–∂—É—Ç—å –º–∞—Ç–∏ —Ä—ñ–∑–Ω—É –¥–æ–≤–∂–∏–Ω—É —ñ –Ω–µ –∑–∞–≤–∂–¥–∏ ¬´–ø–æ–≥–æ–¥–∂–µ–Ω—ñ¬ª –º—ñ–∂ —Å–æ–±–æ—é.

–ü—Ä–∏–∫–ª–∞–¥:
- I am learning AI.
- –Ø –≤–∏–≤—á–∞—é –®–Ü.

–©–æ–± –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ —Ç–∞–∫–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–∑–≤–∏—á–∞–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É ¬´–∫–æ–¥–µ—Ä‚Äì–¥–µ–∫–æ–¥–µ—Ä¬ª:

- –ï–Ω–∫–æ–¥–µ—Ä –ø—Ä–∏–π–º–∞—î –≤—Ö—ñ–¥–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –±—É–¥—å-—è–∫–æ—ó –¥–æ–≤–∂–∏–Ω–∏ —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î —ó—ó –Ω–∞ –∫–æ–º–ø–∞–∫—Ç–Ω–µ –≤–Ω—É—Ç—Ä—ñ—à–Ω—î –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è.
- –î–µ–∫–æ–¥–µ—Ä –ø—Ä–∞—Ü—é—î —è–∫ —É–º–æ–≤–Ω–∞ –º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å: –≤—ñ–Ω –æ—Ç—Ä–∏–º—É—î –∑–∞–∫–æ–¥–æ–≤–∞–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤—ñ–¥ –µ–Ω–∫–æ–¥–µ—Ä–∞ —Ç–∞ –≤–∂–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –≤–∏—Ö—ñ–¥–Ω–æ—ó –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, —â–æ–± –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–∫–µ–Ω.

–¢–∞–∫–∏–º —á–∏–Ω–æ–º, –º–æ–¥–µ–ª—å –º–æ–∂–µ –ø–æ—Å—Ç—É–ø–æ–≤–æ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤–∏—Ö—ñ–¥–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ —ó—ó –¥–æ–≤–∂–∏–Ω–∞ –±—É–¥–µ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –¥–æ–≤–∂–∏–Ω–∏ –≤—Ö—ñ–¥–Ω–æ—ó.

---

class: middle

.center.width-100[![](figures/lec2/seq2seq.svg)]

–†–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä —Å—Ç–∏—Å–∫–∞—é—Ç—å –≤—Ö—ñ–¥–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å $\mathbf{x}\_{1:T}$ –≤ —î–¥–∏–Ω–∏–π –≤–µ–∫—Ç–æ—Ä $v$, –∞ –ø–æ—Ç—ñ–º –≥–µ–Ω–µ—Ä—É—é—Ç—å –≤–∏—Ö—ñ–¥–Ω—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å $\mathbf{y}\_{1:T'}$ –∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—ñ–π–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ—ó –º–æ–¥–µ–ª—ñ:
$$\begin{aligned}
\mathbf{h}\_t &= \phi(\mathbf{x}\_t, \mathbf{h}\_{t-1})\\\\
v &= \mathbf{h}\_{T} \\\\
\mathbf{y}\_{i} &\sim p(\cdot | \mathbf{y}\_{1:i-1}, v).
\end{aligned}$$

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

???

–ü–µ—Ä–µ–∫–ª–∞–¥—ñ—Ç—å –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—É. 

"–ö—ñ—Ç –Ω–µ –ø–µ—Ä–µ–π—à–æ–≤ –≤—É–ª–∏—Ü—é, –±–æ –±—É–≤ –∑–∞–Ω–∞–¥—Ç–æ –≤—Ç–æ–º–ª–µ–Ω–∏–π."

->

"The —Åat didn't cross the street because it was too tired."

---

class: middle

.center.width-80[![](figures/lec2/bottleneck.svg)]

–¶—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ø–µ—Ä–µ–¥–±–∞—á–∞—î, —â–æ —î–¥–∏–Ω–∏–π –≤–µ–∫—Ç–æ—Ä $v$ –º—ñ—Å—Ç–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—Å—ñ—î—ó –≤–∏—Ö—ñ–¥–Ω–æ—ó –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ. –¶–µ —á–∞—Å—Ç–æ —î **—Å–∫–ª–∞–¥–Ω–∏–º –∑–∞–≤–¥–∞–Ω–Ω—è–º** –¥–ª—è –¥–æ–≤–≥–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π.

.footnote[–î–∂–µ—Ä–µ–ª–æ: ULi√®ge - SEGI.]

???

–ù–µ —ñ—Å–Ω—É—î –ø—Ä—è–º–∏—Ö ¬´–∫–∞–Ω–∞–ª—ñ–≤¬ª –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –ª–æ–∫–∞–ª—å–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤—ñ–¥ –≤—Ö—ñ–¥–Ω–æ—ó –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–æ –º—ñ—Å—Ü—è, –¥–µ –≤–æ–Ω–∞ –±—É–¥–µ –∫–æ—Ä–∏—Å–Ω–æ—é –≤ —Ä–µ–∑—É–ª—å—Ç—É—é—á—ñ–π –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ.

–ü—Ä–æ–±–ª–µ–º–∞ —Å—Ö–æ–∂–∞ –Ω–∞ —Ç—É, —â–æ –±—É–ª–∞ –∑ FCN –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–Ω–∏—Ö –∑'—î–¥–Ω–∞–Ω—å: —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑–∞—Å—Ç—Ä—è–≥–ª–∞ –≤ —î–¥–∏–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—ñ $v$.

---

class: blue-slide, middle, center
count: false

.larger-xx[–ú–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏ –ë–∞—Ö–¥–∞–Ω–∞—É <br> (–∞–¥–∏—Ç–∏–≤–Ω–∞ —É–≤–∞–≥–∞)]

---

.center.width-75[![](figures/lec2/Bahdanau.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473).]

???
–ù–∞ —Å—å–æ–≥–æ–¥–Ω—ñ –±—ñ–ª—å—à—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ –∞–±–æ –∑–≥–æ—Ä—Ç–∫–æ–≤—ñ –º–µ—Ä–µ–∂—ñ —É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä. –ù–∞–π–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à—ñ –º–æ–¥–µ–ª—ñ –∑‚Äô—î–¥–Ω—É—é—Ç—å –¥–µ–∫–æ–¥–µ—Ä –∑ –µ–Ω–∫–æ–¥–µ—Ä–æ–º —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏.

---

class: black-slide
background-image: url(figures/lec2/vision.png)
background-size: cover

???
–£–≤–∞–≥–∞ (attention) ‚Äî —Ü–µ –º–µ—Ö–∞–Ω—ñ–∑–º —É –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö, —è–∫–∏–π –¥–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—ñ –≤–∏–±—ñ—Ä–∫–æ–≤–æ —Ñ–æ–∫—É—Å—É–≤–∞—Ç–∏—Å—è –Ω–∞ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö —á–∞—Å—Ç–∏–Ω–∞—Ö –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–µ–≤–Ω–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.


–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è
- –ó–∞–º—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ–± –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤—Å—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –æ–¥–Ω–∞–∫–æ–≤–æ, –º–æ–¥–µ–ª—å ¬´–∑–≤–µ—Ä—Ç–∞—î —É–≤–∞–≥—É¬ª –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏, —â–æ –ø—ñ–¥–≤–∏—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å —ñ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å.
- –ú–æ–∂–µ —Ä–æ–∑–≥–ª—è–¥–∞—Ç–∏—Å—è —è–∫ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä, —è–∫–∏–π –≤–∏–∑–Ω–∞—á–∞—î, —è–∫—ñ —á–∞—Å—Ç–∏–Ω–∏ –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤–∞–∂–ª–∏–≤—ñ –Ω–∞ –¥–∞–Ω–æ–º—É –∫—Ä–æ—Ü—ñ –æ–±—á–∏—Å–ª–µ–Ω—å.

---

class: middle

.center.width-75[![](figures/lec2/eye-coffee.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

.center.width-75[![](figures/lec2/eye-book.svg)]

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –≤–æ–ª—å–æ–≤–∏–π —Å–∏–≥–Ω–∞–ª (–±–∞–∂–∞–Ω–Ω—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –∫–Ω–∏–≥—É), —è–∫–∏–π –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–∞–≤–¥–∞–Ω–Ω—è, —É–≤–∞–≥–∞ —Å–ø—Ä—è–º–æ–≤—É—î—Ç—å—Å—è –Ω–∞ –∫–Ω–∏–≥—É –ø—ñ–¥ –≤–æ–ª—å–æ–≤–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º.

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

.center.width-100[![](figures/lec2/qkv.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

???

1. Query (–∑–∞–ø–∏—Ç) ‚Äî —Ü–µ –≤–µ–∫—Ç–æ—Ä, —è–∫–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Ç–µ, —â–æ –º–∏ —à—É–∫–∞—î–º–æ. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —É –ø–µ—Ä–µ–∫–ª–∞–¥—ñ —Ä–µ—á–µ–Ω–Ω—è ‚Äî —Ü–µ –ø–æ—Ç–æ—á–Ω–µ —Å–ª–æ–≤–æ, —è–∫–µ –º–∏ –Ω–∞–º–∞–≥–∞—î–º–æ—Å—è –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–∏.
2. Key (–∫–ª—é—á) ‚Äî —Ü–µ –≤–µ–∫—Ç–æ—Ä, —è–∫–∏–π –æ–ø–∏—Å—É—î –æ–∑–Ω–∞–∫–∏ (–≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ) –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞ —É –≤—Ö—ñ–¥–Ω—ñ–π –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ. –í—ñ–Ω –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è: ¬´—â–æ –º—ñ—Å—Ç–∏—Ç—å —Ü–µ–π –µ–ª–µ–º–µ–Ω—Ç?¬ª.
3. Value (–∑–Ω–∞—á–µ–Ω–Ω—è) ‚Äî —Ü–µ –≤–µ–∫—Ç–æ—Ä –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é, —è–∫—É –º–∏ —Ä–µ–∞–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ, —è–∫—â–æ –µ–ª–µ–º–µ–Ω—Ç –≤–∏—è–≤–∏–≤—Å—è –≤–∞–∂–ª–∏–≤–∏–º.


## –ê–Ω–∞–ª–æ–≥—ñ—è

–£—è–≤–∏, —â–æ –í–∏ —á–∏—Ç–∞—î—Ç–µ –∫–Ω–∏–≥—É —ñ —à—É–∫–∞—î—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è (Query).

- –¢–≤–æ—î –ø–∏—Ç–∞–Ω–Ω—è ‚Äî —Ü–µ Query.
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–æ–∑–¥—ñ–ª—ñ–≤ —É –∫–Ω–∏–∂—Ü—ñ ‚Äî —Ü–µ Keys (–≤–∫–∞–∑—É—é—Ç—å, –¥–µ –º–æ–∂–µ –±—É—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å).
- –¢–µ–∫—Å—Ç —É —Ü–∏—Ö —Ä–æ–∑–¥—ñ–ª–∞—Ö ‚Äî —Ü–µ Values (—Å–∞–º–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è).

–¢–∏ –ø–æ—Ä—ñ–≤–Ω—é—î—à —Å–≤–æ—î –ø–∏—Ç–∞–Ω–Ω—è —ñ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ (Query –∑ Key), –∑–Ω–∞—Ö–æ–¥–∏—à –Ω–∞–π–±—ñ–ª—å—à –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ä–æ–∑–¥—ñ–ª —ñ –±–µ—Ä–µ—à –∑–≤—ñ–¥—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é (Value).

–í—Ä–µ—à—Ç—ñ-—Ä–µ—à—Ç, —Å–µ–Ω—Å–æ—Ä–Ω—ñ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –æ–±–º–µ–∂—É—é—Ç—å—Å—è –Ω–µ–≤–µ–ª–∏–∫–æ—é —á–∞—Å—Ç–∏–Ω–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –Ω–∞–≤–∫–æ–ª–∏—à–Ω—å–æ–º—É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ. –£ —Ü—å–æ–º—É –ø–æ–ª—è–≥–∞—î —Å—É—Ç—å —É–≤–∞–≥–∏.

---

class: middle

–ú–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏ –º–æ–∂–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –æ–∫—Ä–µ–º–∏—Ö —á–∞—Å—Ç–∏–Ω –≤—Ö—ñ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª—É –¥–æ —á–∞—Å—Ç–∏–Ω –≤–∏—Ö–æ–¥—É, .bold[–≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –¥–∏–Ω–∞–º—ñ—á–Ω–æ]. –¢–æ–±—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–∂–Ω–æ–º—É –∫—Ä–æ—Ü—ñ —Å–∞–º–∞ ¬´–≤–∏—Ä—ñ—à—É—î¬ª, –Ω–∞ —è–∫—ñ —Ç–æ–∫–µ–Ω–∏ –≤—Ö—ñ–¥–Ω–æ—ó –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –∑–≤–µ—Ä–Ω—É—Ç–∏ —É–≤–∞–≥—É –¥–ª—è —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.

–ó–∞ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è–º, —â–æ –∫–æ–∂–µ–Ω –≤–∏—Ö—ñ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω —É—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –∑ –æ–¥–Ω–æ–≥–æ –∞–±–æ –∫—ñ–ª—å–∫–æ—Ö –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤, –¥–µ–∫–æ–¥–µ—Ä –º–∞—î –∑–æ—Å–µ—Ä–µ–¥–∂—É–≤–∞—Ç–∏—Å—è –ª–∏—à–µ –Ω–∞ —Ç–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö, —è–∫—ñ —î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º–∏ –¥–ª—è –≥–µ–Ω–µ—Ä—É–≤–∞–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.


.center[
.width-100[![](figures/lec2/Many-to-many.png)]
]

---

class: middle, 

# –ü—Ä—è–º–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è
.center[
.width-80[![](figures/lec2/inside-rnn.png)]
]
.smaller-x[–î–ª—è –∫–æ–∂–Ω–æ–≥–æ —á–∞—Å–æ–≤–æ–≥–æ –∫—Ä–æ–∫—É  $t$ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è $a^{\langle t \rangle}$ —ñ –≤–∏—Ö—ñ–¥ $y^{\langle t \rangle}$ –≤–∏—Ä–∞–∂–∞—é—Ç—å—Å—è —Ç–∞–∫–∏–º —á–∏–Ω–æ–º: 

$$\boxed{\begin{aligned}a^{\langle t \rangle} &= g\_1(W\_{aa} a^{\langle t-1 \rangle} + W\_{ax}x^{\langle t \rangle} + b\_a) \\\\
\hat y^{\langle t \rangle} &= g\_2(W\_{ya} a^{\langle t \rangle} +  b\_y)
 \end{aligned}}$$

]

.footnote[–î–∂–µ—Ä–µ–ª–æ —Å–ª–∞–π–¥—É: [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) [[video](https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model)], Andrew Ng et al.]

???
–ó–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, —è–∫–∏–π –æ—á—ñ–∫—É—î—Ç—å—Å—è —Ç–∏–ø –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è RNN $\hat y$, —è–∫—â–æ —Ü–µ –∑–∞–¥–∞—á–∞ –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, –≤–∏ –± –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–ª–∏ —Å–∏–≥–º–æ—ó–¥—É –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –≤–∏—Ö–æ–¥—É, –∞–±–æ —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ softmax, —è–∫—â–æ –≤–∏ –≤–∏—Ä—ñ—à—É—î—Ç–µ –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤—É –∑–∞–¥–∞—á—É –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (k –∫–ª–∞—Å—ñ–≤). 

–£ –º–æ–¥—É–ª—ñ RNN –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∞—Ç–∏–≤–∞—Ü—ñ—ó $a^{\langle t \rangle}$ –∑–∞–∑–≤–∏—á–∞–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –≤ —è–∫–æ—Å—Ç—ñ $g\_1$ tanh, —Ü—è –∞—Ç–∏–≤–∞—Ü—ñ–π–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è  –¥–æ–∑–≤–æ–ª—è—î –∑–∞–ø–æ–±—ñ–≥—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ñ –∑–Ω–∏–∫–∞—é—á–æ–≥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞, –ø—Ä–æ —è–∫—É –º–∏ –ø–æ–≥–æ–≤–æ—Ä–∏–º–æ –ø—ñ–∑–Ω—ñ—à–µ.

–î–ª—è –∑–∞–≤–¥–∞–Ω–Ω—è name entity recognition, –Ω–∞ –≤–∏—Ö–æ–¥—ñ –º–æ–¥–µ–ª—ñ –æ—á—ñ–∫—É—î—Ç—å—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ $\hat y$ 0 –∞–±–æ 1, —Ç–æ–º—É –¥—Ä—É–≥—É –∞—Ç–∏–≤–∞—Ü—ñ—é $g\_2$ –º–æ–∂–∞ –æ–±—Ä–∞—Ç–∏ —Å–∏–≥–º–æ—ó–¥—É. 

–¶—ñ —Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–∏–∑–Ω–∞—á–∞—é—Ç—å –ø—Ä—è–º–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –≤ RNN. 

---
class: middle

.center[
.width-100[![](figures/lec2/attention.svg)]
]

.footnote[–î–∂–µ—Ä–µ–ª–æ: ULi√®ge - SEGI.]

---


class: middle

## –ú–∞—à–∏–Ω–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —É–≤–∞–≥–∏

–¢–∞ —Å–∞–º–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ RNN –≤ —è–∫—ñ–π  *–∫–æ–¥–µ—Ä* —Ç–∞ *–¥–µ–∫–æ–¥–µ—Ä* –∑'—î–¥–Ω–∞–Ω—ñ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏.

.center.width-90[![](figures/lec2/seq2seq-attention-details.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

???

–¢–∞ —Å–∞–º–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–µ—Ä–∞-–¥–µ–∫–æ–¥–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ RNN, –∞–ª–µ –∑ –º–µ—Ö–∞–Ω—ñ–∑–º–æ–º —É–≤–∞–≥–∏ –º—ñ–∂ –Ω–∏–º–∏.

---

class: middle

–í—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ Bahdanau et al. (2014), –∫–æ–¥–µ—Ä –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è —è–∫ –¥–≤–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω–∞ –Ω–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ (RNN), —è–∫–∞ –æ–±—á–∏—Å–ª—é—î –≤–µ–∫—Ç–æ—Ä –∞–Ω–æ—Ç–∞—Ü—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞,
$$\mathbf{h}\_j = (\overrightarrow{\mathbf{h}}\_j, \overleftarrow{\mathbf{h}}\_j)$$
–¥–ª—è $j = 1, \ldots, T$, –¥–µ $\overrightarrow{\mathbf{h}}\_j$ —Ç–∞ $\overleftarrow{\mathbf{h}}\_j$ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –ø–æ–∑–Ω–∞—á–∞—é—Ç—å –ø—Ä—è–º—ñ —Ç–∞ –∑–≤–æ—Ä–æ—Ç–Ω—ñ –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ —Å—Ç–∞–Ω–∏ (–∞–∫—Ç–∏–≤–∞—Ü—ñ—ó) –¥–≤–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ—ó RNN.

–ù–∞ –æ—Å–Ω–æ–≤—ñ —Ü—å–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä –æ–±—á–∏—Å–ª—é—î –Ω–æ–≤–∏–π –ø—Ä–æ—Ü–µ—Å $\mathbf{s}\_i$, $i=1, \ldots, T'$, —è–∫–∏–π —Ä–æ–∑–≥–ª—è–¥–∞—î –∑–≤–∞–∂–µ–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è $\mathbf{h}\_j$, –¥–µ .bold[–≤–∞–≥–∏ —î —Ñ—É–Ω–∫—Ü—ñ—è–º–∏ —Å–∏–≥–Ω–∞–ª—É].

$\mathbf{s}\_i$ &mdash; —Ü–µ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–π —Å—Ç–∞–Ω –¥–µ–∫–æ–¥–µ—Ä–∞.

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

.center.width-50[![](figures/lec2/model.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473).]

---

class: middle

–î–ª—è –∑–∞–¥–∞–Ω–∏—Ö $\mathbf{y}\_1, \ldots, \mathbf{y}\_{i-1}$ —Ç–∞ $\mathbf{s}\_1, \ldots, \mathbf{s}\_{i-1}$—Å–ø–æ—á–∞—Ç–∫—É –æ–±—á–∏—Å–ª—ñ—Ç—å –≤–µ–∫—Ç–æ—Ä —É–≤–∞–≥–∏:
$$\mathbf{\alpha}\_{i,j} = \text{softmax}\_j(a(\mathbf{s}\_{i-1}, \mathbf{h}\_j)) = \frac{\exp{(e\_{ij})}}{\sum\_{k=1}^T \exp{(e\_{ik})}}$$
–¥–ª—è $j=1, \ldots, T$, –¥–µ 
- $e\_{ij} = a(\mathbf{s}\_{i-1}, \mathbf{h}\_j)$;
- $a$ &mdash; .bold[—Ñ—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ —É–≤–∞–≥–∏], —è–∫–∞ —Ç—É—Ç –∑–∞–¥–∞–Ω–∞ —è–∫ MLP –∑ –æ–¥–Ω–∏–º –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–º —à–∞—Ä–æ–º $\text{tanh}$;

- $\mathbf{y}$ &mdash; —Ü–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –≤–∏—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏ (–º—ñ—Ç–∫–∏) –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è;
- $\mathbf{s}$ &mdash; —Ü–µ –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —Å—Ç–∞–Ω–∏ –¥–µ–∫–æ–¥–µ—Ä–∞, —è–∫—ñ –æ–Ω–æ–≤–ª—é—é—Ç—å—Å—è –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º.

–ü–æ—Ç—ñ–º –æ–±—á–∏—Å–ª—ñ—Ç—å –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∑–≤–∞–∂–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å $\mathbf{h}\_j$:
$$\mathbf{c}\_i = \sum\_{j=1}^T \alpha\_{i, j} \mathbf{h}\_j.$$

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

???

–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É, —â–æ –≤–∞–≥–∏ —É–≤–∞–≥–∏ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ –∑–º—ñ—Å—Ç—É, –∞ –Ω–µ –≤—ñ–¥ –ø–æ–∑–∏—Ü—ñ—ó –≤ —Ä–µ—á–µ–Ω–Ω—ñ. –¶–µ –æ–∑–Ω–∞—á–∞—î, —â–æ –≤–æ–Ω–∏ –¥—ñ—é—Ç—å —è–∫ —Ñ–æ—Ä–º–∞ *–∞–¥—Ä–µ—Å–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–º—ñ—Å—Ç—É*.

---

class: middle

–¢–µ–ø–µ—Ä –º–æ–¥–µ–ª—å –º–æ–∂–µ –∑—Ä–æ–±–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ $\mathbf{y}\_i$:
$$
\begin{aligned}
\mathbf{s}\_i &= f(\mathbf{s}\_{i-1}, y\_{i-1}, c\_i)  \\\\
\mathbf{y}\_i &\sim g(\mathbf{y}\_{i-1}, \mathbf{s}\_i, \mathbf{c}\_i),
\end{aligned}
$$
–¥–µ $f$ &mdash; [Gated Recurrent Unit (GRU)](https://arxiv.org/pdf/1409.1259).

–¶–µ **—É–≤–∞–≥–∞ –¥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É**, –¥–µ $\mathbf{s}\_{i-1}$ –≤–∏–∑–Ω–∞—á–∞—î, —â–æ —à—É–∫–∞—Ç–∏ –≤ $\mathbf{h}\_1, \ldots, \mathbf{h}\_{T}$ –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è $\mathbf{s}\_i$ —Ç–∞ –∑—Ä–∞–∑–∫–∞ $\mathbf{y}\_i$.

---

class: middle

.center.width-100[![](figures/lec2/translation-attention.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473).]

???

- Source = English
- Target = French

---

class: blue-slide, middle, center
count: false

.larger-xx[–®–∞—Ä —É–≤–∞–≥–∏]

---

class: middle

–ú–µ—Ö–∞–Ω—ñ–∑–º–∏ —É–≤–∞–≥–∏ –º–æ–∂–Ω–∞ –∑–∞–≥–∞–ª–æ–º –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–º —á–∏–Ω–æ–º.

–î–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∞–±–æ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø–∏—Ç—É (Query) $\mathbf{q} \in \mathbb{R}^{q}$, —Ç–µ–Ω–∑–æ—Ä–∞ –∫–ª—é—á—ñ–≤ (Key) $\mathbf{K} \in \mathbb{R}^{m \times k}$ —Ç–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –∑–Ω–∞—á–µ–Ω—å $\mathbf{V} \in \mathbb{R}^{m \times v}$ —à–∞—Ä —É–≤–∞–≥–∏ –æ–±—á–∏—Å–ª—é—î –≤–µ–∫—Ç–æ—Ä –≤–∏—Ö–æ–¥—É $\mathbf{y} \in \mathbb{R}^{v}$
–∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é: $$\mathbf{y} = \sum\_{i=1}^m \text{softmax}\_i(a(\mathbf{q}, \mathbf{K}\_i; \theta)) \mathbf{V}\_i,$$
–¥–µ $a : \mathbb{R}^q \times \mathbb{R}^k \to \mathbb{R}$ ‚Äî —Å–∫–∞–ª—è—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ —É–≤–∞–≥–∏, 
$\theta$ &mdash; –Ω–∞–≤—á–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏, —è–∫—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—é—Ç—å Query —ñ Key —É —á–∏—Å–ª–æ–≤—É –æ—Ü—ñ–Ω–∫—É —É–≤–∞–≥–∏.

---

class: middle

.center.width-100[![](figures/lec2/attention-output.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

## –ê–¥–∏—Ç–∏–≤–Ω–∞ —É–≤–∞–≥–∞

–£ –≤–∏–ø–∞–¥–∫—É, –∫–æ–ª–∏ –∑–∞–ø–∏—Ç–∏ —Ç–∞ –∫–ª—é—á—ñ —î –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ä—ñ–∑–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏, –º–∏ –º–æ–∂–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∞–¥–∏—Ç–∏–≤–Ω—É —É–≤–∞–≥—É —è–∫ —Ñ—É–Ω–∫—Ü—ñ—é –æ—Ü—ñ–Ω–∫–∏.

–î–ª—è $\mathbf{q} \in \mathbb{R}^{q}$ —Ç–∞ $\mathbf{k} \in \mathbb{R}^{k}$ —Ñ—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ **–∞–¥–∏—Ç–∏–≤–Ω–æ—ó —É–≤–∞–≥–∏** –º–∞—î –≤–∏–≥–ª—è–¥:
$$a(\mathbf{q}, \mathbf{k}) = \mathbf{w}_v^T \tanh(\mathbf{W}\_q^T \mathbf{q} + \mathbf{W}\_k^T \mathbf{k})$$
–¥–µ $\mathbf{w}_v \in \mathbb{R}^h$, $\mathbf{W}_q \in \mathbb{R}^{q \times h}$ —Ç–∞ $\mathbf{W}_k \in \mathbb{R}^{k \times h}$ &mdash; –Ω–∞–≤—á–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

---

class: middle

## –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–π —Å–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ —É–≤–∞–≥–∏


–ö–æ–ª–∏ –∑–∞–ø–∏—Ç–∏ —Ç–∞ –∫–ª—é—á—ñ —î –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –æ–¥–Ω–∞–∫–æ–≤–æ—ó –¥–æ–≤–∂–∏–Ω–∏ $d$, –º–∏ –º–æ–∂–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–π —Å–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ —É–≤–∞–≥–∏ —è–∫ —Ñ—É–Ω–∫—Ü—ñ—é –æ—Ü—ñ–Ω–∫–∏.

–î–ª—è $\mathbf{q} \in \mathbb{R}^{d}$ —Ç–∞ $\mathbf{k} \in \mathbb{R}^{d}$ —Ñ—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ **–º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –¥–æ–±—É—Ç–∫—É —É–≤–∞–≥–∏** –º–∞—î –≤–∏–≥–ª—è–¥:
$$a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^T \mathbf{k}}{\sqrt{d}}$$

---

class: middle

–î–ª—è $n$ –∑–∞–ø–∏—Ç—ñ–≤ $\mathbf{Q} \in \mathbb{R}^{n \times d}$, –∫–ª—é—á—ñ–≤ $\mathbf{K} \in \mathbb{R}^{m \times d}$ —ñ –∑–Ω–∞—á–µ–Ω—å $\mathbf{V} \in \mathbb{R}^{m \times v}$ —à–∞—Ä **–º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –¥–æ–±—É—Ç–∫—É —É–≤–∞–≥–∏** –æ–±—á–∏—Å–ª—é—î –≤–∏—Ö—ñ–¥–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä:
$$\mathbf{Y} = \underbrace{\text{softmax}\left(\frac{\mathbf{QK}^T}{\sqrt{d}}\right)}\_{\text{–º–∞—Ç—Ä–∏—Ü—è —É–≤–∞–≥–∏}\, \mathbf{A}}\mathbf{V} \in \mathbb{R}^{n \times v}$$

---

class: middle

.center.width-80[![](figures/lec2/dot-product.png)]

C–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ –¥–≤–æ—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤ &mdash; —Ü–µ —á–∏—Å–ª–æ, —è–∫–µ —Ä—ñ–≤–Ω–µ –¥–æ–±—É—Ç–∫—É –¥–æ–≤–∂–∏–Ω —Ü–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤ –Ω–∞ –∫–æ—Å–∏–Ω—É—Å –∫—É—Ç–∞ –º—ñ–∂ –Ω–∏–º–∏ (–∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ –¥–≤–æ—Ö –Ω–µ –Ω—É–ª—å–æ–≤–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤).

–û—Ç–∂–µ, –º–∞—Ç—Ä–∏—Ü—è $\mathbf{QK}^T$ —î **–º–∞—Ç—Ä–∏—Ü–µ—é –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ** –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ —Ç–∞ –∫–ª—é—á–∞–º–∏.

.footnote[–î–∂–µ—Ä–µ–ª–æ: ULi√®ge - SEGI.]

---

class: middle

.center.width-100[![](figures/lec2/qkv-maps.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: middle

–£ —Å—É—á–∞—Å–Ω–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π –∑–∞–ø–∏—Ç–∏, –∫–ª—é—á—ñ —Ç–∞ –∑–Ω–∞—á–µ–Ω–Ω—è —î –ª—ñ–Ω—ñ–π–Ω–∏–º–∏ —Ñ—É–Ω–∫—Ü—ñ—è–º–∏ –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö.

–ù–µ—Ö–∞–π –º–∞—Ç—Ä–∏—Ü—ñ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–∞—é—Ç—å –≤–∏–≥–ª—è–¥ $\mathbf{W}\_q \in \mathbb{R}^{d \times x}$, $\mathbf{W}\_k \in \mathbb{R}^{d \times x'}$, —ñ $\mathbf{W}\_v \in \mathbb{R}^{v \times x'}$, –∞ —Ç–∞–∫–æ–∂ –¥–≤—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫ $\mathbf{X} \in \mathbb{R}^{n \times x}$ —ñ $\mathbf{X}' \in \mathbb{R}^{m \times x'}$, –∑ —è–∫–∏—Ö —Ñ–æ—Ä–º—É—é—Ç—å—Å—è –∑–∞–ø–∏—Ç–∏ (Queries), –∫–ª—é—á—ñ (Keys) —Ç–∞ –∑–Ω–∞—á–µ–Ω–Ω—è (Values), —Ç–æ–¥—ñ –º–∞—î–º–æ:
$$\begin{aligned} 
\mathbf{Q} &= \mathbf{X} \mathbf{W}\_q^T \in \mathbb{R}^{n \times d} \\\\
\mathbf{K} &= \mathbf{X'} \mathbf{W}\_k^T \in \mathbb{R}^{m \times d} \\\\
\mathbf{V} &= \mathbf{X'} \mathbf{W}\_v^T \in \mathbb{R}^{m \times v}
\end{aligned}$$

- $\mathbf{X}$ ‚Äî –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —Å—Ç–∞–Ω–∏ –¥–µ–∫–æ–¥–µ—Ä–∞ (—Ñ–æ—Ä–º—É—é—Ç—å Queries).
- $\mathbf{X'}$ ‚Äî –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —Å—Ç–∞–Ω–∏ –∫–æ–¥–µ—Ä–∞ (—Ñ–æ—Ä–º—É—é—Ç—å Keys —ñ Values).

---

class: middle

## Self-attention ([—Å–∞–º–æ—É–≤–∞–≥–∞](https://developers.google.com/machine-learning/crash-course/llm/transformers?hl=uk#what_is_self-attention))

–ö–æ–ª–∏ –∑–∞–ø–∏—Ç–∏, –∫–ª—é—á—ñ —Ç–∞ –∑–Ω–∞—á–µ–Ω–Ω—è –æ—Ç—Ä–∏–º—É—é—Ç—å—Å—è –∑ –æ–¥–Ω–∏—Ö —ñ —Ç–∏—Ö —Å–∞–º–∏—Ö –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –º–µ—Ö–∞–Ω—ñ–∑–º —É–≤–∞–≥–∏ –Ω–∞–∑–∏–≤–∞—î—Ç—å—Å—è **—Å–∞–º–æ—É–≤–∞–≥–æ—é**.

–î–ª—è –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–æ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –¥–æ–±—É—Ç–∫—É —É–≤–∞–≥–∏ —à–∞—Ä —Å–∞–º–æ—É–≤–∞–≥–∏ –æ—Ç—Ä–∏–º—É—î—Ç—å—Å—è, –∫–æ–ª–∏ $\mathbf{X} = \mathbf{X}'$.

–¢–æ–º—É —Å–∞–º–æ—É–≤–∞–≥–∞ –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏—Å—è —è–∫ –∑–≤–∏—á–∞–π–Ω–∏–π —à–∞—Ä –ø—Ä—è–º–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è, –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ –ø–æ–≤–Ω–æ–∑–≤‚Äô—è–∑–Ω–∏—Ö –∞–±–æ –∑–≥–æ—Ä—Ç–∫–æ–≤–∏—Ö —à–∞—Ä—ñ–≤.

<br>
.center.width-60[![](figures/lec2/self-attention-layer.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: ULi√®ge - SEGI.]

---

class: middle 

## CNNs vs. RNNs vs. self-attention

.center.width-80[![](figures/lec2/cnn-rnn-self-attention.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

???

–ü–æ—Ä—ñ–≤–Ω—è—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –±—É–¥—ñ–≤–µ–ª—å–Ω–∏—Ö –±–ª–æ–∫—ñ–≤.

CNNs:
- –°–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏: –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞, –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –∫–æ—Ä–æ—Ç—à–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
- –û–±–º–µ–∂–µ–Ω–Ω—è: –æ–±–º–µ–∂–µ–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ, –≤–∏–∑–Ω–∞—á–µ–Ω–µ —Ä–æ–∑–º—ñ—Ä–æ–º —è–¥—Ä–∞
- –ö–ª—é—á–æ–≤–∏–π –º–æ–º–µ–Ω—Ç: –¥–æ–≤–∂–∏–Ω–∞ —à–ª—è—Ö—É O(log_k(n)) (–∑ –æ–±'—î–¥–Ω–∞–Ω–Ω—è–º) –æ–∑–Ω–∞—á–∞—î, —â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ—Ö–æ–¥–∏—Ç—å —á–µ—Ä–µ–∑ –º–µ–Ω—à–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å, –Ω—ñ–∂ —É RNN

RNNs:
- –°–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏: –ø—Ä–∏—Ä–æ–¥–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π, –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
- –û–±–º–µ–∂–µ–Ω–Ω—è: –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å—Ç–≤–æ—Ä—é—î –≤—É–∑—å–∫—ñ –º—ñ—Å—Ü—è
- –ö–ª—é—á–æ–≤–∏–π –º–æ–º–µ–Ω—Ç: –¥–æ–≤–∂–∏–Ω–∞ —à–ª—è—Ö—É O(n) –æ–∑–Ω–∞—á–∞—î –≤—Ç—Ä–∞—Ç—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –¥–æ–≤–≥–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—è—Ö

Self-Attention:
- –°–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏: –ø—Ä—è–º–µ –∑'—î–¥–Ω–∞–Ω–Ω—è –º—ñ–∂ –±—É–¥—å-—è–∫–∏–º–∏ –ø–æ–∑–∏—Ü—ñ—è–º–∏, –¥–æ–≤–∂–∏–Ω–∞ —à–ª—è—Ö—É O(1)
- –û–±–º–µ–∂–µ–Ω–Ω—è: —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å O(n¬≤) —Å—Ç–∞—î –Ω–µ–ø—Ä–∏–π–Ω—è—Ç–Ω–æ—é –¥–ª—è –¥—É–∂–µ –¥–æ–≤–≥–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
- –ö–ª—é—á–æ–≤–∏–π –º–æ–º–µ–Ω—Ç: —Ü—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å —î —Ü—ñ–Ω–æ—é, —è–∫—É –º–∏ –ø–ª–∞—Ç–∏–º–æ –∑–∞ –≤–µ–ª–∏–∫—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è

---

class: middle

.center.width-100[![](figures/lec2/complexity.png)]

–¥–µ $n$ ‚Äî –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, $d$ ‚Äî —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≤–±—É–¥–æ–≤—É–≤–∞–Ω–Ω—è, –∞ $k$ ‚Äî —Ä–æ–∑–º—ñ—Ä —è–¥—Ä–∞ –∑–≥–æ—Ä—Ç–∫–∏.

???

–Ø–∫ –∑–∞–∑–Ω–∞—á–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü—ñ 1, —à–∞—Ä —Å–∞–º–æ—É–≤–∞–≥–∏ –∑'—î–¥–Ω—É—î –≤—Å—ñ –ø–æ–∑–∏—Ü—ñ—ó –∑ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –≤–∏–∫–æ–Ω–∞–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π, —Ç–æ–¥—ñ —è–∫ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω–∏–π —à–∞—Ä –≤–∏–º–∞–≥–∞—î O(n) –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π. –ó —Ç–æ—á–∫–∏ –∑–æ—Ä—É –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ—ó —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ, —à–∞—Ä–∏ —Å–∞–º–æ—É–≤–∞–≥–∏ —î —à–≤–∏–¥—à–∏–º–∏ –∑–∞ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ —à–∞—Ä–∏, –∫–æ–ª–∏ –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ $n$ —î –º–µ–Ω—à–æ—é –∑–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è $d$, —â–æ –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –º–∞—î –º—ñ—Å—Ü–µ —É –≤–∏–ø–∞–¥–∫—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —Ä–µ—á–µ–Ω—å, —è–∫–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ –Ω–∞–π—Å—É—á–∞—Å–Ω—ñ—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É, —Ç–∞–∫–∏—Ö —è–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è word-piece[38] —Ç–∞ byte-pair [31]. 

–û–¥–∏–Ω –∑–≥–æ—Ä—Ç–∫–æ–≤–∏–π —à–∞—Ä –∑ —è–¥—Ä–æ–º $k < n$ –Ω–µ –∑'—î–¥–Ω—É—î –≤—Å—ñ –ø–∞—Ä–∏ –≤—Ö—ñ–¥–Ω–∏—Ö —ñ –≤–∏—Ö—ñ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü—ñ–π. –î–ª—è —Ü—å–æ–≥–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ç–µ–∫ –∑ $O(n/k)$ –∑–≥–æ—Ä—Ç–∫–æ–≤–∏—Ö —à–∞—Ä—ñ–≤ —É –≤–∏–ø–∞–¥–∫—É —Å—É—Å—ñ–¥–Ω—ñ—Ö —è–¥–µ—Ä, –∞–±–æ $O(logk(n))$ —É –≤–∏–ø–∞–¥–∫—É —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö –∑–≥–æ—Ä—Ç–æ–∫ [18], —â–æ –∑–±—ñ–ª—å—à—É—î –¥–æ–≤–∂–∏–Ω—É –Ω–∞–π–¥–æ–≤—à–∏—Ö —à–ª—è—Ö—ñ–≤ –º—ñ–∂ –±—É–¥—å-—è–∫–∏–º–∏ –¥–≤–æ–º–∞ –ø–æ–∑–∏—Ü—ñ—è–º–∏ –≤ –º–µ—Ä–µ–∂—ñ. –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏, —è–∫ –ø—Ä–∞–≤–∏–ª–æ, –¥–æ—Ä–æ–∂—á—ñ –∑–∞ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ —à–∞—Ä–∏ –≤ $k$ —Ä–∞–∑—ñ–≤. 

---

class: middle

## –ü—Ä–∏–∫–ª–∞–¥

–©–æ–± –ø—Ä–æ—ñ–ª—é—Å—Ç—Ä—É–≤–∞—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –º–µ—Ö–∞–Ω—ñ–∑–º—É —É–≤–∞–≥–∏, —Ä–æ–∑–≥–ª—è–Ω–µ–º–æ –ø—Ä–æ—Å—Ç—É –∑–∞–¥–∞—á—É –∑ 1d –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—è–º–∏, —â–æ —Å–∫–ª–∞–¥–∞—é—Ç—å—Å—è –∑ –¥–≤–æ—Ö —Ç—Ä–∏–∫—É—Ç–Ω–∏–∫—ñ–≤ —ñ –¥–≤–æ—Ö –ø—Ä—è–º–æ–∫—É—Ç–Ω–∏–∫—ñ–≤. –¶—ñ–ª—å–æ–≤–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å (Target) —É—Å–µ—Ä–µ–¥–Ω—é—î –≤–∏—Å–æ—Ç–∏ –≤ –∫–æ–∂–Ω—ñ–π –ø–∞—Ä—ñ —Ñ—ñ–≥—É—Ä.

.center.width-100[![](figures/lec2/toy1.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: middle

.center.width-80[![](figures/lec2/toy1-training.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: middle

–ú–∏ –º–æ–∂–µ–º–æ –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—É, —â–æ–± —Ä–æ–∑–≥–ª—è–Ω—É—Ç–∏ –º–µ—Ç—É (Target), –¥–µ –ø–∞—Ä–∞–º–∏ –¥–ª—è —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è —î –¥–≤—ñ –∫—Ä–∞–π–Ω—ñ —Ñ—ñ–≥—É—Ä–∏ –ø—Ä–∞–≤–æ—Ä—É—á —ñ –ª—ñ–≤–æ—Ä—É—á.

.center.width-100[![](figures/lec2/toy2.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: middle

–û—á—ñ–∫—É—î—Ç—å—Å—è, —â–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –±—É–¥–µ –Ω–∏–∑—å–∫–æ—é, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –Ω–µ–∑–¥–∞—Ç–Ω—ñ—Å—Ç—å —à–∞—Ä—É —Å–∞–º–æ—É–≤–∞–≥–∏ –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ñ –∞–±–æ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ–∑–∏—Ü—ñ—ó. –î—ñ–π—Å–Ω–æ, —Å–∞–º–æ—É–≤–∞–≥–∞ —î —ñ–Ω–≤–∞—Ä—ñ–∞–Ω—Ç–Ω–æ—é –¥–æ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏:
$$\begin{aligned}
\mathbf{y} &= \sum\_{i=1}^m \text{softmax}\_i\left(\frac{\mathbf{q}^T{\mathbf{K}^T\_{i}}}{\sqrt{d}}\right) \mathbf{V}\_{i}\\\\
&= \sum\_{i=1}^m \text{softmax}\_{i}\left(\frac{\mathbf{q}^T{\mathbf{K}^T\_{\sigma(i)}}}{\sqrt{d}}\right) \mathbf{V}\_{\sigma(i)}
\end{aligned}$$
–¥–ª—è –±—É–¥—å-—è–∫–æ—ó –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ $\sigma$ –ø–∞—Ä –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–Ω—è.

(–°–∞–º–æ—É–≤–∞–≥–∞ —Ç–∞–∫–æ–∂ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–∞ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤—Ü—ñ $\sigma$ –∑–∞–ø–∏—Ç—ñ–≤.)

---

class: middle

.center.width-80[![](figures/lec2/toy2-training.png)]

–û–¥–Ω–∞–∫ —Ü—é –ø—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–∞ –≤–∏—Ä—ñ—à–∏—Ç–∏, –Ω–∞–¥–∞–≤—à–∏ –ø–æ–∑–∏—Ü—ñ–π–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è —è–≤–Ω–æ —à–∞—Ä—É —É–≤–∞–≥–∏.

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: blue-slide, middle, center
count: false

.larger-xx[–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏]

---

class: middle

Vaswani —Ç–∞ —ñ–Ω. (2017) –∑–∞–ø—Ä–æ–ø–æ–Ω—É–≤–∞–ª–∏ –ø—ñ—Ç–∏ —â–µ –¥–∞–ª—ñ: –∑–∞–º—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–µ—Ö–∞–Ω—ñ–∑–º—É —É–≤–∞–≥–∏ —è–∫ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –∑–≥–æ—Ä—Ç–∫–æ–≤–∏—Ö —ñ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö —à–∞—Ä—ñ–≤, –≤–æ–Ω–∏ —Ä–æ–∑—Ä–æ–±–∏–ª–∏ –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫ .bold[—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞], —â–æ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –≤–∏–∫–ª—é—á–Ω–æ –∑ —à–∞—Ä—ñ–≤ —É–≤–∞–≥–∏.

–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±—É–≤ —Ä–æ–∑—Ä–æ–±–ª–µ–Ω–∏–π –¥–ª—è –∑–∞–≤–¥–∞–Ω–Ω—è –ø–µ—Ä–µ–∫–ª–∞–¥—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å-–ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å, –∞–ª–µ –≤ –¥–∞–Ω–∏–π —á–∞—Å —Ü–µ–π –±–ª–æ–∫ —î –∫–ª—é—á–æ–≤–∏–º –µ–ª–µ–º–µ–Ω—Ç–æ–º –Ω–∞–π—Å—É—á–∞—Å–Ω—ñ—à–∏—Ö –ø—ñ–¥—Ö–æ–¥—ñ–≤ –¥–æ –±—ñ–ª—å—à–æ—Å—Ç—ñ –∑–∞–≤–¥–∞–Ω—å, —â–æ —Å—Ç–æ—Å—É—é—Ç—å—Å—è –æ–±—Ä–æ–±–∫–∏ –Ω–∞–±–æ—Ä—ñ–≤ –∞–±–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π.

.footnote[–î–∂–µ—Ä–µ–ª–æ: Francois Fleuret, [Deep Learning](https://fleuret.org/dlc/), UNIGE/EPFL.]

---

class: middle

.center.width-65[![](./figures/lec2/transformers.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v7).]

---

.grid[
.kol-1-1[ 
.circle.center.width-40[![](./figures/lec2/polosuhyn.png)]
.larger-xx.bold.center[.width-10[![](./figures/lec2/ua.gif)] –Ü–ª–ª—è –ü–æ–ª–æ—Å—É—Ö—ñ–Ω] 
<span style="display:block; margin:10px 0;"></span>
.bold.center[–ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç <br> ¬´–•–∞—Ä–∫—ñ–≤—Å—å–∫–∏–π –ø–æ–ª—ñ—Ç–µ—Ö–Ω—ñ—á–Ω–∏–π —ñ–Ω—Å—Ç–∏—Ç—É—Ç¬ª]]
]

.success[–°–ø—ñ–≤–∞–≤—Ç–æ—Ä –Ω–∞—É–∫–æ–≤–æ—ó —Ä–æ–±–æ—Ç–∏ ‚ÄúAttention Is All You Need‚Äù, —è–∫–∞ –∑–∞–ø–æ—á–∞—Ç–∫—É–≤–∞–ª–∞ –µ–ø–æ—Ö—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤ —É —à—Ç—É—á–Ω–æ–º—É —ñ–Ω—Ç–µ–ª–µ–∫—Ç—ñ.]

.footnote[LinkedIn: [Illia Polosukhin](https://www.linkedin.com/in/illia-polosukhin-77b6538/), [[–£–∫—Ä–∞—ó–Ω–µ—Ü—å –Ü–ª–ª—è –ü–æ–ª–æ—Å—É—Ö—ñ–Ω –±—É–≤ —Å–µ—Ä–µ–¥ 8 –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ Google, —è–∫—ñ –∑–∞–∫–ª–∞–¥–∞–ª–∏ –æ—Å–Ω–æ–≤–∏ —Å—É—á–∞—Å–Ω–æ–≥–æ –®–Ü. –Ø–∫ —Ü–µ –±—É–ª–æ]](https://dev.ua/news/ukrainets-zakladav-osnovy-shi-v-google-1710942264).]
---

class: middle

## Scaled dot-product attention

The first building block of the transformer architecture is a scaled dot-production attention module
$$\text{attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d\_k}}\right) \mathbf{V}$$
where the $1/\sqrt{d\_k}$ scaling is used to keep the (softmax's) temperature constant across different choices of the query/key dimension $d\_k$.

---

class: middle

.center.width-55[![](figures/lec2/multi-head-attention.svg)]

## Multi-head attention

The transformer projects the queries, keys and values $h=8$ times with distinct linear projections to $d\_k=64$, $d\_k=64$ and $d\_v=64$ dimensions respectively.
$$
\begin{aligned}
\text{multihead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &= \text{concat}\left(\mathbf{H}\_1, \ldots, \mathbf{H}\_h\right) \mathbf{W}^O\\\\
\mathbf{H}\_i &= \text{attention}(\mathbf{Q}\mathbf{W}\_i^Q, \mathbf{K}\mathbf{W}\_i^K, \mathbf{V}\mathbf{W}\_i^V)
\end{aligned}
$$
with
$$\mathbf{W}\_i^Q \in \mathbb{R}^{d\_\text{model} \times d\_k}, \mathbf{W}\_i^K \in \mathbb{R}^{d\_\text{model} \times d\_k}, \mathbf{W}\_i^V \in \mathbb{R}^{d\_\text{model} \times d\_v}, \mathbf{W}\_i^O \in \mathbb{R}^{hd\_v \times d\_\text{model}}$$

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

## Encoder-decoder architecture

The transformer model is composed of:
- An encoder that combines $N=6$ modules, each composed of a multi-head attention sub-module, and a (per-component) one-hidden-layer MLP, with residual pass-through and layer normalization. All sub-modules and embedding layers produce outputs of dimension $d\_\text{model}=512$.
- A decoder that combines $N=6$ modules similar to the encoder, but using masked self-attention to prevent positions from attending to subsequent positions. In addition, the decoder inserts a third sub-module which performs multi-head attention over the output of the encoder stack.

---

class: middle

.center.width-60[![](figures/lec2/transformer.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

.center.width-90[![](figures/lec2/transformer-decoding-1.gif)]

The encoders start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors $\mathbf{K}$ and $\mathbf{V}$ passed to the decoders.

.footnote[–î–∂–µ—Ä–µ–ª–æ: Jay Alammar, [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).]

---

class: middle

.center.width-90[![](figures/lec2/transformer-decoding-2.gif)]

Each step in the decoding phase produces an output token, until a special symbol is reached indicating the completion of the transformer decoder's output.

The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. 

.footnote[–î–∂–µ—Ä–µ–ª–æ: Jay Alammar, [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).]

---

class: middle

In the decoder:
- The first masked self-attention sub-module is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions.
- The second multi-head attention sub-module works just like multi-head self-attention, except it creates its query matrix from the layer below it, and takes the keys and values matrices from the output of the encoder stack.

.footnote[–î–∂–µ—Ä–µ–ª–æ: Jay Alammar, [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).]

---

class: middle

## Positional encoding

Positional information is provided through an **additive** positional encoding of the same dimension $d\_\text{model}$ as the internal representation and is of the form
$$
\begin{aligned}
\text{PE}\_{t,2i} &= \sin\left(\frac{t}{10000^{\frac{2i}{d\_\text{model}}}}\right) \\\\
\text{PE}\_{t,2i+1} &= \cos\left(\frac{t}{10000^{\frac{2i}{d\_\text{model}}}}\right).
\end{aligned}
$$

After adding the positional encoding, words will be closer to each other based on the similarity of their meaning and their relative position in the sentence, in the $d\_\text{model}$-dimensional space.

Alternatively, the model can also learn the positional encoding.

???

All words of input sequence are fed to the network with no special order or position; in contrast, in an RNN, the ùëõ-th word is fed at step ùëõ, and in a CNN, it is fed to specific input indices. 

---

class: middle

.width-100[![](figures/lec2/positional-encoding.png)]

.center[128-dimensional positonal encoding for a sentence with the maximum length of 50. Each row represents the embedding vector.]

---

class: middle

## Machine translation

The transformer architecture was first designed for machine translation and tested on English-to-German and English-to-French translation tasks.

.center[

.width-100[![](figures/lec2/transformer-attention-example.png)]

Self-attention layers learned that "it" could refer<br> to different entities, in different contexts.
  
]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), 2017.]

---

class: middle

.center[

.width-100[![](figures/lec2/attention-plots.png)]

Attention maps extracted from the multi-head attention modules<br> show how input tokens relate to output tokens.
  
]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer).]

---

class: middle

## Decoder-only transformers

The decoder-only transformer has become the de facto architecture for large language models $p(\mathbf{x}\_t | \mathbf{x}\_{1:t-1})$.

These models are trained with self-supervised learning, where the target sequence is the same as the input sequence, but shifted by one token to the right.

.center.width-80[![](./figures/lec2/gpt-decoder-only.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]
  
---

class: middle, center

([demo](https://poloclub.github.io/transformer-explainer/))

---

class: middle

Historically, GPT-1 was first pre-trained and then fine-tuned on downstream tasks.

.width-100[![](figures/lec2/gpt.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: Radford et al., [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), 2018.]

---

class: middle

## Scaling laws

Transformer language model performance improves smoothly as we increase the model size, the dataset size, and amount of compute used for training. 

For optimal performance, all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two.

.center.width-100[![](./figures/lec2/scaling-power-law.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Kaplan et al](https://arxiv.org/pdf/2001.08361.pdf), 2020.]

---

class: middle

Large models also enjoy better sample efficiency than small models.
- Larger models require less data to achieve the same performance.
- The optimal model size shows to grow smoothly with the amount of compute available for training.

<br>
.center.width-100[![](./figures/lec2/scaling-sample-conv.png)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Kaplan et al](https://arxiv.org/pdf/2001.08361.pdf), 2020.]

---

class: middle

## Conversational agents

.center.width-70[![](./figures/lec2/chatgpt.png)]

All modern conversational agents are based on the same transformer models, scaled up to billions of parameters, trillions of training tokens, and thousands of petaflop/s-days of compute.

---

class: middle
count: false

# Transformers for images

---

class: middle

The transformer architecture was first designed for sequences, but it can be adapted to process images.

The key idea is to reshape the input image into a sequence of patches, which are then processed by a transformer encoder. This architecture is known as the .bold[vision transformer] (ViT).

---

class: middle

.center.width-80[![](./figures/lec2/vit.svg)]

.footnote[–î–∂–µ—Ä–µ–ª–æ: [Dive Into Deep Learning](https://d2l.ai), 2023.]

---

class: middle

- The input image is divided into non-overlapping patches, which are then linearly embedded into a sequence of vectors.
- The sequence of vectors is then processed by a transformer encoder, which outputs a sequence of vectors.
- Training the vision transformer can be done with supervised or self-supervised learning.
  
---

class: end-slide, center
count: false

.larger-xxxx[üèÅ]
